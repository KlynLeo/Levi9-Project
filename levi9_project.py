# -*- coding: utf-8 -*-
"""Levi9_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v7oSQEV8nlHEz-NHBVAmAHuQIuS7_ylj
"""

!pip install datasets
!pip install bs4
!pip install langchain sentence-transformers faiss-cpu transformers datasets accelerate huggingface_hub
!pip install -U langchain-community

from datasets import load_dataset
from bs4 import BeautifulSoup
import pandas as pd
import re

# Load
ds = load_dataset("habedi/stack-exchange-dataset", split="train")
filtered_ds = ds.filter(lambda example: "data structures" in example["tags"] and example["label"] == 1)

# Clean
def clean_text(text):
    text = BeautifulSoup(str(text), "html.parser").get_text()
    text = text.replace("$", "")
    text = re.sub(r"\\log", "log", text)
    text = re.sub(r"\\sqrt{([^}]+)}", r"sqrt(\1)", text)
    text = re.sub(r"\\frac{([^}]+)}{([^}]+)}", r"(\1/\2)", text)
    text = re.sub(r"\\[a-zA-Z]+", "", text)
    text = re.sub(r"[^\w\s.,!?()^/<>:]", "", text)  # <-- păstrează ':'
    text = re.sub(r"[ \t]+", " ", text)  # Înlocuiește doar spații și taburi cu un singur spațiu
    return text


df = filtered_ds.to_pandas()


def truncate_text(text, max_words=450):
    return ' '.join(text.split()[:max_words])

# Combine question + answer into retrievable context
def make_context(row):
    return f"Q: {str(row['title'])}\nA: {str(row['body'])}"

df["text"] = df.apply(make_context, axis=1)
df["text"] = df["text"].apply(clean_text).apply(truncate_text)

df['text'][0]

from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.docstore.document import Document

embedding_model = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

docs = [Document(page_content=row["text"]) for _, row in df.iterrows()]
vectorstore = FAISS.from_documents(docs, embedding_model)

query = "How do van Emde Boas trees work?"
results = vectorstore.similarity_search_with_score(query, k=5)

print("\nDocuments with scores:")
for doc, score in results:
    print(f"Document: {doc.page_content}")
    print(f"Score: {score}\n")

from transformers import T5Tokenizer, AutoModelForSeq2SeqLM, pipeline
from langchain.llms import HuggingFacePipeline
from huggingface_hub import login

# Login to Hugging Face
login("")#remove secret token

# Model ID for T5
model_id = "google/flan-t5-large"

# Load Tokenizer and Model
tokenizer = T5Tokenizer.from_pretrained(model_id)
model = AutoModelForSeq2SeqLM.from_pretrained(model_id, device_map="auto", torch_dtype="auto")

# Create a text2text-generation pipeline for T5
t5_pipeline = pipeline(
    "text2text-generation",  # This is the correct pipeline for T5 models
    model=model,
    tokenizer=tokenizer,
    max_length=512,  # You can adjust this based on your use case
    do_sample=True,
    temperature=0.1,
)

# Wrap it in a HuggingFacePipeline for LangChain
llm = HuggingFacePipeline(pipeline=t5_pipeline)

from langchain.prompts import PromptTemplate

template = """You are a helpful assistant for answering data structure questions.

Context:
{context}

Question:
{question}

Answer:"""


prompt = PromptTemplate(
    input_variables=["context", "question"],
    template=template
)
filled_prompt = prompt.format(
    context="Q: Are AVL trees always balanced? A: AVL trees are height-balanced, but not necessarily weight-balanced...",
    question="Are all AVL trees balanced?"
)

print(filled_prompt)

from langchain.chains import RetrievalQA

retriever = vectorstore.as_retriever(search_kwargs={"k": 2})

rag_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type="stuff",  # or "map_reduce" if your context is too large
    chain_type_kwargs={"prompt": prompt}
)

query = "Why is btree search O(log n)?"
response = rag_chain.invoke(query)
context = rag_chain.retriever.get_relevant_documents(query)  # Get the documents retrieved as context

print("Context used for generation:")
for doc in context:
    print(f"Context: {doc.page_content}")  # Display the context for each retrieved document

print("\nGenerated answer:")
print(response)  # Display the generated answer